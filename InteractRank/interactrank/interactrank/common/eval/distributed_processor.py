from __future__ import annotations

from typing import TYPE_CHECKING
from typing import List
from typing import TypeVar
from typing_extensions import Protocol
from typing import Generic
from typing import Optional
from typing import cast
import time
import torch
import pickle
import torch.distributed as dist
from interactrank.common.utils.model_utils import AllGatherWithGrad
from interactrank.common.utils.utils import send_to_device
from torch import Tensor

import logging
from abc import abstractmethod

T = TypeVar("T")
Result = TypeVar("Result")
PartialResult = TypeVar("PartialResult")
logger = logging.getLogger(__name__)


class DistributedProcessor(Protocol[PartialResult, Result]):
    """
    A processor describing a computation that be run in parallel on different workers. The class defines a common
    protocol for describing any such process so that it can then be run by a distibuted runner.

    """

    @abstractmethod
    def initialize(self, worker_id: int, **kwargs) -> None:
        """
        Initializes the processor for worker_id. A distributed runner will call this function on different workers
        to initialize the processor state for that worker.

        :param worker_id: Id of the worker executing the processor
        :param kwargs: Additional args passed from the runner
        """
        ...

    @property
    @abstractmethod
    def is_initialized(self) -> bool:
        """
        :return: True if the processor is initialized. Can be used in the run function to assert the processor is in
        correct state
        """
        ...

    @abstractmethod
    def run(self) -> PartialResult:
        """
        Run the actual computation chunk to produce a partial result on the initialized worker.
        Since the partial result is synced between processed which might involve pickling and network transfer,
        it is desirable to keep the partial results small in size.
        """
        ...

    @abstractmethod
    def close(self) -> None:
        """
        Clean up any resources that the processor might have acquired during the computation
        """
        ...

    @abstractmethod
    def merge_partial_results(self, partial_results: List[PartialResult]) -> Result:
        """
        Merges different partial results obtained by running the processor on different workers. The distributed runner
        runs this function only one worker after gathering partial results from all workers. The order of the partial
        results is not deterministic and any implementation should not rely on it.

        :param partial_results: Partial results generated by running the processor on different workers
        :return: Result obtained by combining different partial results
        """
        ...

    def consume_result(self, result: Result) -> None:
        """
        Consume the result of the computation

        :param result: Final result of the computation described by the processor
        """
        ...

class DistributedProcessorRunner(Generic[PartialResult, Result]):
    """
    Abstract class for a distributed runner that can run the DistributedProcessor on multiple workers in parallel
    """

    def __init__(self, processor: DistributedProcessor[PartialResult, Result]) -> None:
        """
        :param processor: Processor to run on multiple workers
        """
        self.processor = processor

    @abstractmethod
    def __call__(self, **kwargs) -> Result:
        """
        :param kwargs: Args to pass to the processor initializer in addition to the worker_id
        :return: Result of the computation
        """
        ...


# Batched implementation of all_gather_object
def all_gather_object(obj: T, batch_size: int = 50_000_000) -> List[T]:
    def _object_to_tensor(obj_: T) -> Tensor:
        # Send to CPU to prevent pickling CUDA tensors (can cause memory leaks).
        obj_ = send_to_device(obj_, device="cpu", ignore_error=True)
        # Ensure all tensors have successfully moved to cpu before pickling
        torch.cuda.synchronize()
        buffer = pickle.dumps(obj_, protocol=4)
        tensor = torch.frombuffer(buffer, dtype=torch.uint8)
        return tensor

    def _tensor_to_object(tensor: Tensor) -> T:
        buf = tensor.cpu().numpy().tobytes()
        out = pickle.loads(buf)
        return out

    t = _object_to_tensor(obj)
    current_device = torch.device("cuda", torch.cuda.current_device())
    # find max size to determine how many batches must be gathered
    max_size = torch.tensor([t.numel()]).to(device=current_device)
    dist.all_reduce(max_size, op=dist.ReduceOp.MAX)
    all_out = [[] for _ in range(dist.get_world_size())]

    for offset in range(0, cast(int, max_size.cpu().item()), batch_size):
        for append_to, tensor_chunk in zip(
            all_out,
            AllGatherWithGrad.apply(t[offset : offset + batch_size].to(device=current_device)),
        ):
            append_to.append(tensor_chunk.cpu())
    final_res = []
    for ser_obj in all_out:
        final_res.append(_tensor_to_object(torch.cat(ser_obj, dim=0)))
    return final_res


class TorchDistributedRunner(DistributedProcessorRunner[PartialResult, Result]):
    """
    Distributed Runner which runs the distributed processor using torch's distributed APIs.
    The runner does not create workers and assumes that the workers are already created and the runner is run on
    each worker
    """

    def __init__(
        self,
        processor: DistributedProcessor[PartialResult, Result],
        results_in_all_procs: bool = False,
        verbosity: int = 2,
        gather_batch_size: int = 50_000_000,
    ) -> None:
        """
        :param processor: Processor to run on multiple workers
        :param results_in_all_procs: Flag to make the processor run results available in all participating processes.
            Works only for NCCL backend
        :param verbosity: (0, 1, or 2). 0 -> no logging, 1 -> only log in root process, 2 -> log in all processes
        :param gather_batch_size: number of bytes to gather at once from each process in all_gather_object.
            This will only be used if the distributed backend is NCCL
        """
        if verbosity not in (0, 1, 2):
            raise ValueError("unrecognized verbosity level")
        assert not results_in_all_procs or (not dist.is_initialized() or dist.get_backend() == dist.Backend.NCCL), (
            "Can only return in all processes with NCCL backend"
        )
        self._verbosity = verbosity
        self.world_size = dist.get_world_size() if dist.is_initialized() else 1
        self.rank = dist.get_rank() if dist.is_initialized() else 0
        self._gather_batch_size = gather_batch_size
        self.return_results_in_all_procs = results_in_all_procs
        super().__init__(processor)

    def __call__(self, **kwargs) -> Optional[Result]:
        """
        :param kwargs: Args to pass to the processor initializer in addition to the worker_id
        :return: Result of the computation
        """
        # Clear the CUDA memory cache before starting evaluation
        # As of Pytorch 1.13, we have found that clearing the cache reduces increase in max memory used and hence OOMs
        torch.cuda.empty_cache()

        # Initialize the processor with the given rank and arguments
        self.processor.initialize(self.rank, **kwargs)

        start_time = time.time()

        # Run the processor and store the partial result
        # If an exception is raised, close the processor and re-raise the exception
        # This is done to ensure that the processor is always closed and resources are released
        # even if an exception is raised
        # Need to understand how this would behave if the one of the processes fail and the rest are killed by the controller
        try:
            partial_result = self.processor.run()
            self.processor.close()
        except Exception as e:
            logger.error(f"Error in processor run: {e}")
            self.processor.close()
            raise e

        # Clear the CUDA memory cache before collecting results across all GPUs
        torch.cuda.empty_cache()

        # Wait for all processes to complete
        if dist.is_initialized():
            dist.barrier()
        should_log = self._verbosity == 2 or ((self._verbosity == 1) and self.rank == 0)

        if should_log:
            logger.info(f"Time taken by processor ({self.rank}): {time.time() - start_time:.2f} seconds")
        start_time = time.time()

        if dist.is_initialized():
            partial_results = [None] * self.world_size

            if dist.get_backend() == dist.Backend.GLOO:
                dist.gather_object(partial_result, partial_results if self.rank == 0 else None, dst=0)

            elif dist.get_backend() == dist.Backend.NCCL:
                partial_results = all_gather_object(partial_result, batch_size=self._gather_batch_size)

        else:
            partial_results = [partial_result]

        if should_log:
            logger.info(f"Time taken by sync ({self.rank}): {time.time() - start_time:.2f} seconds")

        if self.rank == 0 or self.return_results_in_all_procs:
            if should_log:
                logger.info("Merging partial results")
            start_time = time.time()

            # Merge the partial results to obtain the final result and consume the result using the processor
            result = self.processor.merge_partial_results(partial_results)
            if should_log:
                logger.info(
                    f"Merging partial results completed, time taken ({self.rank}): {time.time() - start_time:.2f} seconds"
                )
            start_time = time.time()
            self.processor.consume_result(result)
            if should_log:
                logger.info(
                    f"Consuming result completed, time taken ({self.rank}): {time.time() - start_time:.2f} seconds"
                )

        else:
            result = None

        # Clear the CUDA memory cache after eval completion
        if should_log:
            logger.info(f"Clearing CUDA memory cache ({self.rank})")
        torch.cuda.empty_cache()
        if should_log:
            logger.info(f"Cleared CUDA memory cache ({self.rank})")

        # Wait for all processes to complete
        if dist.is_initialized():
            dist.barrier()

        return result
